import akshare as ak
import pandas as pd
import numpy as np
import datetime
from scipy.stats import linregress
import time

def get_etf_list_sina():
    """ä»…ä½¿ç”¨æ–°æµªæ¥å£è·å–å…¨å¸‚åœºETFåˆ—è¡¨"""
    print("æ­£åœ¨è¿æ¥æ–°æµªè´¢ç»æ¥å£è·å–å…¨å¸‚åœºETFåˆ—è¡¨...")
    try:
        df = ak.fund_etf_category_sina(symbol="ETFåŸºé‡‘")
        if 'symbol' in df.columns:
            df.rename(columns={'symbol': 'ä»£ç ', 'name': 'åç§°'}, inplace=True)
        return df
    except Exception as e:
        print(f"è·å–ETFåˆ—è¡¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ: {e}")
        return pd.DataFrame()

def get_etf_hist_sina_safe(code, start_str, end_str):
    """è·å–å†å²æ•°æ®ï¼Œä¿®å¤äº†å‰ç¼€è¯†åˆ«é€»è¾‘ï¼Œçº¯æ–°æµªæº"""
    # ã€åº”ç”¨äº†ä½ ä¿®å¤çš„é€»è¾‘ã€‘ï¼šè¯†åˆ«æ˜¯å¦è‡ªå¸¦ sh/sz å‰ç¼€
    sina_symbol = code if str(code).startswith('s') else (f"sh{code}" if str(code).startswith(('5', '7')) else f"sz{code}")
    
    try:
        hist_df = ak.fund_etf_hist_sina(symbol=sina_symbol)
        
        if hist_df is not None and not hist_df.empty:
            hist_df.rename(columns={'date':'æ—¥æœŸ', 'open':'å¼€ç›˜', 'high':'æœ€é«˜', 'low':'æœ€ä½', 'close':'æ”¶ç›˜', 'volume':'æˆäº¤é‡'}, inplace=True)
            # å¼ºåˆ¶è½¬åŒ–ä¸ºæ•°å€¼æ ¼å¼
            for col in['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']:
                hist_df[col] = pd.to_numeric(hist_df[col], errors='coerce')
                
            hist_df.dropna(subset=['æ”¶ç›˜', 'æˆäº¤é‡'], inplace=True)
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            hist_df['æ—¥æœŸ'] = pd.to_datetime(hist_df['æ—¥æœŸ']).dt.strftime('%Y%m%d')
            hist_df = hist_df[(hist_df['æ—¥æœŸ'] >= start_str) & (hist_df['æ—¥æœŸ'] <= end_str)].reset_index(drop=True)
            
            return hist_df
    except Exception as e:
        pass
        
    return None

def calculate_grid_metrics(df):
    """è®¡ç®—ç½‘æ ¼æŒ‡æ ‡ï¼ŒåŠ å…¥æµåŠ¨æ€§ä¸æŒ¯å¹…åŒé‡ç¡¬æ€§è¿‡æ»¤"""
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # ------------------ ä¸€ç¥¨å¦å†³ 1ï¼šæµåŠ¨æ€§è¿‡æ»¤ ------------------
    # è®¡ç®—æ¯æ—¥æˆäº¤é¢ (æ–°æµªçš„æˆäº¤é‡å•ä½é€šå¸¸ä¸ºè‚¡ï¼Œæˆäº¤é¢ = æˆäº¤é‡ * æ”¶ç›˜ä»·)
    df['æˆäº¤é¢'] = df['æˆäº¤é‡'] * df['æ”¶ç›˜']
    
    # å–æœ€è¿‘ 20 ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®æµ‹ç®—è¿‘æœŸçœŸå®æµåŠ¨æ€§
    recent_20_days = df.tail(20)
    if len(recent_20_days) < 20:
        return None
        
    avg_turnover = recent_20_days['æˆäº¤é¢'].mean()
    
    # ã€æ ¸å¿ƒæ¡ä»¶ã€‘ï¼šè¿‘20æ—¥å¹³å‡æ—¥æˆäº¤é¢å¿…é¡» > 5000ä¸‡ (50,000,000å…ƒ)
    # è¿™ä¹Ÿä¾§é¢ä¿è¯äº†åŸºé‡‘è§„æ¨¡(AUM)ç»å¤§éƒ¨åˆ†åœ¨å‡ äº¿ä»¥ä¸Šï¼Œæ— æ¸…ç›˜é£é™©
    if avg_turnover < 50000000:
        return None
    # ------------------------------------------------------------
    
    # è®¡ç®— MA120
    df['MA120'] = df['æ”¶ç›˜'].rolling(window=120).mean()
    df = df.dropna().reset_index(drop=True)
    if len(df) < 120: return None
        
    # ------------------ ä¸€ç¥¨å¦å†³ 2ï¼šæŒ¯å¹…è¿‡æ»¤ --------------------
    df['å‰æ”¶ç›˜'] = df['æ”¶ç›˜'].shift(1)
    df.loc[0, 'å‰æ”¶ç›˜'] = df.loc[0, 'å¼€ç›˜']
    df['æ—¥æŒ¯å¹…'] = (df['æœ€é«˜'] - df['æœ€ä½']) / df['å‰æ”¶ç›˜']
    avg_daily_amplitude = df['æ—¥æŒ¯å¹…'].mean() * 100 
    
    # ã€æ ¸å¿ƒæ¡ä»¶ã€‘ï¼šå¦‚æœå¹³å‡æ—¥æŒ¯å¹…ä½äº 1.5%ï¼Œä¸å¤Ÿç½‘æ ¼å·®ä»·ï¼Œæ·˜æ±°ï¼
    if avg_daily_amplitude < 1.5:
        return None
    # ------------------------------------------------------------

    # --- é•¿æœŸç¨³å®šæ€§ ---
    ma120_cv = df['MA120'].std() / df['MA120'].mean()
    x = np.arange(len(df))
    y = df['MA120'].values
    slope, _, _, _, _ = linregress(x, y)
    trend_slope_pct = abs(slope) / df['MA120'].mean() * 100 
    
    # --- éœ‡è¡çº¯åº¦ (Choppiness Index) ---
    path_length = abs(df['æ”¶ç›˜'] - df['å‰æ”¶ç›˜']).sum()
    net_displacement = abs(df['æ”¶ç›˜'].iloc[-1] - df['æ”¶ç›˜'].iloc[0])
    choppiness = 1.0 - (net_displacement / (path_length + 0.0001))
    
    # --- ç»¼åˆè¯„åˆ† V2 ---
    penalty = ma120_cv + trend_slope_pct
    grid_score = (avg_daily_amplitude * choppiness) / (penalty + 0.1)
    
    return {
        'ç½‘æ ¼ç»¼åˆè¯„åˆ†': round(grid_score, 2),
        'å¹³å‡æ—¥æŒ¯å¹…(%)': round(avg_daily_amplitude, 2),
        'éœ‡è¡çº¯åº¦(0-1)': round(choppiness, 3),
        'è¿‘20æ—¥å‡æˆäº¤é¢': f"{avg_turnover / 100000000:.2f} äº¿", # æ ¼å¼åŒ–ä¸ºâ€œäº¿â€
        'MA120å˜å¼‚ç³»æ•°': round(ma120_cv, 4),
        'è¶‹åŠ¿æ–œç‡æƒ©ç½š': round(trend_slope_pct, 4)
    }

def scan_etf_for_grid(min_years=3):
    """ä¸»ç¨‹åºï¼šå»ºè®®å¹´é™è®¾ä¸º3å¹´å³å¯ï¼Œå¤ªé•¿ä¼šè¿‡æ»¤æ‰å¾ˆå¤šä¼˜è´¨æ–°ç§‘æŠ€ETF"""
    etf_spot = get_etf_list_sina() 
    if etf_spot.empty:
        return
        
    total_etf = len(etf_spot)
    print(f"\nå…±å‘ç° {total_etf} åªETFã€‚å¼€å§‹æé€Ÿæ‰«æï¼Œå·²å¼€å¯ã€æ—¥å‡æˆäº¤é¢>5000ä¸‡ã€‘ä¸ã€æ—¥æŒ¯å¹…>1.5%ã€‘åŒé‡ç¡¬è¿‡æ»¤...")
    
    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=min_years * 365)
    start_str = start_date.strftime("%Y%m%d")
    end_str = end_date.strftime("%Y%m%d")
    
    results =[]
    count = 0
    valid_count = 0
    
    for index, row in etf_spot.iterrows():
        count += 1
        code = str(row['ä»£ç ']).strip()
        name = str(row['åç§°']).strip()
        
        # æ’é™¤ä¸€çœ¼å‡çš„å“ç§
        if any(keyword in name for keyword in["è´§å¸", "å€º", "ç†è´¢", "é»„é‡‘", "æ·»ç›Š", "å¿«çº¿"]):
            continue
            
        hist_df = get_etf_hist_sina_safe(code, start_str, end_str)
        
        # æ•°æ®é•¿åº¦å¤Ÿä¸å¤Ÿ (1å¹´çº¦240äº¤æ˜“æ—¥)
        if hist_df is not None and len(hist_df) >= (min_years * 240) * 0.9:
            metrics = calculate_grid_metrics(hist_df)
            
            if metrics: # å¦‚æœmetricsä¸ä¸ºNoneï¼Œè¯´æ˜é€šè¿‡äº†æµåŠ¨æ€§å’ŒæŒ¯å¹…çš„é­”é¬¼æµ‹è¯•
                metrics['ä»£ç '] = code
                metrics['åç§°'] = name
                results.append(metrics)
                valid_count += 1
        
        if count % 50 == 0 or count == total_etf:
            print(f"è¿›åº¦ï¼šå·²å¤„ç† {count}/{total_etf}ï¼Œå½“å‰å¹¸å­˜çš„ç½‘æ ¼åœ£ä½“ï¼š{valid_count} åª...")
            
        time.sleep(0.01) # å¾®å°å»¶è¿Ÿä¿æŠ¤æ¥å£

    res_df = pd.DataFrame(results)
    if res_df.empty:
        print("\næ‰«æç»“æŸï¼šæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ‰€æœ‰è‹›åˆ»æ¡ä»¶çš„ETFã€‚")
        return None
        
    # æ’åº
    res_df = res_df.sort_values('ç½‘æ ¼ç»¼åˆè¯„åˆ†', ascending=False).reset_index(drop=True)
    
    # è°ƒæ•´åˆ—æ˜¾ç¤ºé¡ºåº
    cols =['ä»£ç ', 'åç§°', 'ç½‘æ ¼ç»¼åˆè¯„åˆ†', 'å¹³å‡æ—¥æŒ¯å¹…(%)', 'éœ‡è¡çº¯åº¦(0-1)', 'è¿‘20æ—¥å‡æˆäº¤é¢', 'MA120å˜å¼‚ç³»æ•°', 'è¶‹åŠ¿æ–œç‡æƒ©ç½š']
    final_df = res_df[cols]
    
    print("\n================== ğŸ¯ æ‰«æå®Œæˆï¼é€šè¿‡ã€èµ„é‡‘é¢+æŠ€æœ¯é¢ã€‘åŒé‡è€ƒéªŒçš„æœ€ç»ˆ TOP 15 ==================")
    print(final_df.head(15).to_string(index=False))
    
    # å¯¼å‡ºå®Œæ•´çš„æ¸…æ´—ç»“æœ
    final_df.to_csv("å…¨å¸‚åœºç½‘æ ¼ETFç»ˆæé€‰å“è¡¨.csv", index=False, encoding="utf-8-sig")
    print("\nå®Œæ•´ç»“æœå·²ä¿å­˜è‡³ï¼šå…¨å¸‚åœºç½‘æ ¼ETFç»ˆæé€‰å“è¡¨.csv")
    
    return final_df

if __name__ == "__main__":
    # æ³¨æ„ï¼šæˆ‘ä»¬å°†å†å²è€ƒå¯ŸæœŸè®¾ä¸º 3 å¹´ã€‚
    # å› ä¸ºå¾ˆå¤šé«˜å¼¹æ€§çš„ç¡¬ç§‘æŠ€ã€åŒ»è¯ETFæ˜¯è¿‘ä¸‰å¹´ä¸Šå¸‚çš„ï¼Œ5å¹´/10å¹´ä¼šæŠŠæœ€å¥½çš„å“ç§è¿‡æ»¤æ‰ã€‚
    scan_etf_for_grid(min_years=3)